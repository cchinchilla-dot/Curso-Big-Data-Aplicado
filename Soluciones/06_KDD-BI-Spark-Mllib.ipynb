{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-TWexVGZyhl"
      },
      "source": [
        "# KDD, Inteligencia de Negocios y Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTQjKNL1Zyhn"
      },
      "source": [
        "## Configuración inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOIrJHWsZ4DY",
        "outputId": "806f9c00-ae49-48e4-93b6-b84bfcc8db93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDQlRoTJZyho"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MLlibExamples\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKJcwZOeZyhp"
      },
      "source": [
        "## Ejemplos de código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d0LFP7mZyhp"
      },
      "source": [
        "### Ejemplo 1: Creación de un DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9bcgsDMZyhq",
        "outputId": "a0a6aae2-d9a7-444e-d858-4fc8a2ba621f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- f1: double (nullable = true)\n",
            " |-- f2: double (nullable = true)\n",
            " |-- f3: double (nullable = true)\n",
            " |-- f4: double (nullable = true)\n",
            " |-- label: long (nullable = true)\n",
            " |-- derived_feature: integer (nullable = false)\n",
            " |-- category_encoded: integer (nullable = true)\n",
            " |-- target_regression: double (nullable = true)\n",
            "\n",
            "+---+--------+----+----+----+----+-----+---------------+----------------+------------------+\n",
            "| id|category|  f1|  f2|  f3|  f4|label|derived_feature|category_encoded| target_regression|\n",
            "+---+--------+----+----+----+----+-----+---------------+----------------+------------------+\n",
            "|  1|       A|0.95|0.51|0.43|0.02|    0|              1|               0| 4.169006099562851|\n",
            "|  2|       B|0.29|0.12|0.18|0.81|    0|              0|               1|1.6186491801554297|\n",
            "|  3|       C|0.91|0.14|0.19|0.57|    1|              1|               2|2.8703515223407723|\n",
            "|  4|       D|0.23|0.43|0.51|0.57|    1|              0|               3|2.8409677091421788|\n",
            "|  5|       B| 0.8|0.45|0.73|0.97|    0|              1|               1|4.6105710748547075|\n",
            "+---+--------+----+----+----+----+-----+---------------+----------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Número total de filas: 10000\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necesarias\n",
        "from pyspark.sql.functions import rand, when, col, expr, lit\n",
        "import random\n",
        "\n",
        "# Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ExtendedDatasetCreation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Definir categorías y características\n",
        "categories = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "features = [\"f1\", \"f2\", \"f3\", \"f4\"]\n",
        "\n",
        "# Generar datos ampliados\n",
        "extended_data = []\n",
        "for i in range(1, 10001):  # Generamos 10,000 registros\n",
        "    category = random.choice(categories)\n",
        "    feature_values = [round(random.uniform(0, 1), 2) for _ in range(len(features))]\n",
        "    label = random.choice([0, 1])  # Etiqueta binaria para clasificación\n",
        "    extended_data.append((i, category) + tuple(feature_values) + (label,))\n",
        "\n",
        "# Crear DataFrame\n",
        "columns = [\"id\", \"category\"] + features + [\"label\"]\n",
        "df = spark.createDataFrame(extended_data, columns)\n",
        "\n",
        "# Añadir algunas características derivadas y valores nulos\n",
        "df = df.withColumn(\"derived_feature\",\n",
        "                   when(col(\"f1\") + col(\"f2\") > 1, 1).otherwise(0))\n",
        "\n",
        "df = df.withColumn(\"category_encoded\",\n",
        "                   when(col(\"category\") == \"A\", 0)\n",
        "                   .when(col(\"category\") == \"B\", 1)\n",
        "                   .when(col(\"category\") == \"C\", 2)\n",
        "                   .when(col(\"category\") == \"D\", 3)\n",
        "                   .when(col(\"category\") == \"E\", 4))\n",
        "\n",
        "# Introducir algunos valores nulos aleatoriamente\n",
        "df = df.withColumn(\"f1\", when(rand() > 0.95, None).otherwise(col(\"f1\")))\n",
        "\n",
        "# Añadir una columna numérica para regresión\n",
        "df = df.withColumn(\"target_regression\", expr(\"f1 * 2 + f2 * 3 + f3 * 1.5 + f4 * 0.5 + rand() * 0.1\"))\n",
        "\n",
        "# Mostrar el esquema y algunas filas de muestra\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "# Guardar el DataFrame para su uso posterior si es necesario\n",
        "# df.write.parquet(\"path/to/extended_dataset.parquet\")\n",
        "\n",
        "print(f\"Número total de filas: {df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD3a4VErZyhq"
      },
      "source": [
        "### Ejemplo 2: Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdDwpJoFZyhq",
        "outputId": "ab6de061-2351-4dca-baf5-e0054fa2bdc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo 1: DataFrame con características imputadas y escaladas\n",
            "+---+--------+----------+----------+----------+----------+--------------------+-----+\n",
            "| id|category|f1_imputed|f2_imputed|f3_imputed|f4_imputed|      scaledFeatures|label|\n",
            "+---+--------+----------+----------+----------+----------+--------------------+-----+\n",
            "|  1|       A|      0.95|      0.51|      0.43|      0.02|[3.38661067681834...|    0|\n",
            "|  2|       B|      0.29|      0.12|      0.18|      0.81|[1.03380746976559...|    0|\n",
            "|  3|       C|      0.91|      0.14|      0.19|      0.57|[3.24401654305757...|    1|\n",
            "|  4|       D|      0.23|      0.43|      0.51|      0.57|[0.81991626912444...|    1|\n",
            "|  5|       B|       0.8|      0.45|      0.73|      0.97|[2.85188267521544...|    0|\n",
            "+---+--------+----------+----------+----------+----------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Columna scaledFeatures contiene:\n",
            "+-----------------------------------------------------------------------------+\n",
            "|scaledFeatures                                                               |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|[3.386610676818342,1.772527114281085,1.4943674951198571,0.06968648401539354] |\n",
            "|[1.033807469765599,0.41706520336025527,0.6255491840036611,2.8223026026234384]|\n",
            "|[3.24401654305757,0.48657607058696456,0.6603019164483089,1.9860647944387155] |\n",
            "|[0.8199162691244408,1.494483645374248,1.77238935467704,1.9860647944387155]   |\n",
            "|[2.851882675215446,1.5639945126009573,2.536949468459292,3.3797944747465865]  |\n",
            "+-----------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Imputer, StandardScaler\n",
        "\n",
        "# Manejo de valores faltantes\n",
        "imputer = Imputer(inputCols=[\"f1\", \"f2\", \"f3\", \"f4\"], outputCols=[\"f1_imputed\", \"f2_imputed\", \"f3_imputed\", \"f4_imputed\"])\n",
        "imputer_model = imputer.fit(df)\n",
        "df_imputed = imputer_model.transform(df)\n",
        "\n",
        "# Escalado de características\n",
        "assembler = VectorAssembler(inputCols=[\"f1_imputed\", \"f2_imputed\", \"f3_imputed\", \"f4_imputed\"], outputCol=\"features\")\n",
        "df_assembled = assembler.transform(df_imputed)\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "scaler_model = scaler.fit(df_assembled)\n",
        "df_scaled = scaler_model.transform(df_assembled)\n",
        "\n",
        "print(\"Ejemplo 1: DataFrame con características imputadas y escaladas\")\n",
        "df_scaled.select(\"id\", \"category\", \"f1_imputed\", \"f2_imputed\", \"f3_imputed\", \"f4_imputed\", \"scaledFeatures\", \"label\").show(5)\n",
        "\n",
        "print(\"Columna scaledFeatures contiene:\")\n",
        "df_scaled.select(\"scaledFeatures\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWA8oCLzZyhr"
      },
      "source": [
        "### Ejemplo 3: Entrenamiento de un modelo de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FFg2crbZyhr",
        "outputId": "cd16b86c-4dd5-446e-e8df-d2caa7626f23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ejemplo 2: Predicciones de Regresión Logística\n",
            "+---+-----+----------+--------------------+\n",
            "| id|label|prediction|         probability|\n",
            "+---+-----+----------+--------------------+\n",
            "|  3|    1|       0.0|[0.52114115040225...|\n",
            "|  7|    0|       0.0|[0.52102406546541...|\n",
            "|  9|    1|       1.0|[0.47788162697766...|\n",
            "| 14|    0|       0.0|[0.50226771693924...|\n",
            "| 20|    0|       1.0|[0.47764981063960...|\n",
            "+---+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "AUC: 0.4879246798574064\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline,\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder\n",
        "\n",
        "# Preprocesamiento\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "encoder = OneHotEncoder(inputCols=[\"categoryIndex\"], outputCols=[\"categoryVec\"])\n",
        "assembler = VectorAssembler(inputCols=[\"categoryVec\", \"f1\", \"f2\", \"f3\", \"f4\"], outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Modelo\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, encoder, assembler, lr])\n",
        "\n",
        "# Dividir los datos\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Entrenamiento\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Predicciones\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "print(\"\\nEjemplo 2: Predicciones de Regresión Logística\")\n",
        "predictions.select(\"id\", \"label\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# Evaluación\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXBH-KcndhkG"
      },
      "source": [
        "# Ejemplo 3: Clasificación multiclase (usando 'category' como etiqueta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shzrJ5dMen-4",
        "outputId": "9cc39092-e16a-4f43-c6cd-6011c65abe10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- f1: double (nullable = true)\n",
            " |-- f2: double (nullable = true)\n",
            " |-- f3: double (nullable = true)\n",
            " |-- f4: double (nullable = true)\n",
            "\n",
            "\n",
            "Distribución de categorías:\n",
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|       E|  208|\n",
            "|       B|  204|\n",
            "|       D|  203|\n",
            "|       C|  183|\n",
            "|       A|  202|\n",
            "+--------+-----+\n",
            "\n",
            "\n",
            "Resumen estadístico de las características numéricas:\n",
            "+-------+-----------------+--------+------------------+------------------+------------------+-----------------+\n",
            "|summary|               id|category|                f1|                f2|                f3|               f4|\n",
            "+-------+-----------------+--------+------------------+------------------+------------------+-----------------+\n",
            "|  count|             1000|    1000|              1000|              1000|              1000|             1000|\n",
            "|   mean|            499.5|    NULL| 5.231319999999999|           5.05741| 4.976270000000002|          5.13917|\n",
            "| stddev|288.8194360957494|    NULL|2.9147252839815083|2.8791746801364337|2.8878921566097655|2.868944833132132|\n",
            "|    min|                0|       A|               0.0|               0.0|              0.01|              0.0|\n",
            "|    max|              999|       E|              10.0|              9.99|              9.99|             9.99|\n",
            "+-------+-----------------+--------+------------------+------------------+------------------+-----------------+\n",
            "\n",
            "\n",
            "Muestra de los datos:\n",
            "+---+--------+----+----+----+----+\n",
            "| id|category|  f1|  f2|  f3|  f4|\n",
            "+---+--------+----+----+----+----+\n",
            "|  0|       B|8.98|6.12|0.25|4.08|\n",
            "|  1|       A|7.31|4.94|2.99|7.84|\n",
            "|  2|       B| 7.5|2.45|5.36|5.42|\n",
            "|  3|       E|6.99|2.08|1.57| 2.0|\n",
            "|  4|       D| 9.7|3.77|9.28|9.95|\n",
            "+---+--------+----+----+----+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Número total de filas: 1000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, when\n",
        "import random\n",
        "\n",
        "# Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewMulticlassDataset\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Definir categorías\n",
        "categories = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
        "\n",
        "# Función para generar datos de ejemplo\n",
        "def generate_example(id):\n",
        "    category = random.choice(categories)\n",
        "    f1 = round(random.uniform(0, 10), 2)\n",
        "    f2 = round(random.uniform(0, 10), 2)\n",
        "    f3 = round(random.uniform(0, 10), 2)\n",
        "    f4 = round(random.uniform(0, 10), 2)\n",
        "    return (id, category, f1, f2, f3, f4)\n",
        "\n",
        "# Generar datos\n",
        "data = [generate_example(i) for i in range(1000)]  # 1000 ejemplos\n",
        "\n",
        "# Crear DataFrame\n",
        "columns = [\"id\", \"category\", \"f1\", \"f2\", \"f3\", \"f4\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Mostrar algunas estadísticas del DataFrame\n",
        "print(\"Esquema del DataFrame:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nDistribución de categorías:\")\n",
        "df.groupBy(\"category\").count().show()\n",
        "\n",
        "print(\"\\nResumen estadístico de las características numéricas:\")\n",
        "df.describe().show()\n",
        "\n",
        "print(\"\\nMuestra de los datos:\")\n",
        "df.show(5)\n",
        "\n",
        "print(f\"\\nNúmero total de filas: {df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dD3_jxoddrd",
        "outputId": "631d0b5e-5733-4feb-ed95-5834ad0d01cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ejemplo 3: Clasificación multiclase\n",
            "Accuracy: 0.1813186813186813\n",
            "Matriz de confusión:\n",
            "+-------------+----------+-----+\n",
            "|categoryIndex|prediction|count|\n",
            "+-------------+----------+-----+\n",
            "|          2.0|       0.0|   23|\n",
            "|          1.0|       1.0|   15|\n",
            "|          0.0|       1.0|   10|\n",
            "|          1.0|       0.0|   21|\n",
            "|          3.0|       1.0|   17|\n",
            "|          2.0|       1.0|   18|\n",
            "|          0.0|       0.0|   17|\n",
            "|          4.0|       3.0|    3|\n",
            "|          4.0|       0.0|   16|\n",
            "|          3.0|       3.0|    1|\n",
            "|          0.0|       3.0|    2|\n",
            "|          3.0|       0.0|   23|\n",
            "|          4.0|       1.0|   14|\n",
            "|          2.0|       3.0|    1|\n",
            "|          1.0|       3.0|    1|\n",
            "+-------------+----------+-----+\n",
            "\n",
            "\n",
            "Algunas predicciones:\n",
            "+--------+-------------+----------+--------------------+\n",
            "|category|categoryIndex|prediction|         probability|\n",
            "+--------+-------------+----------+--------------------+\n",
            "|       B|          2.0|       1.0|[0.20653306285646...|\n",
            "|       C|          4.0|       0.0|[0.21387365470132...|\n",
            "|       D|          3.0|       0.0|[0.21803917820107...|\n",
            "|       C|          4.0|       0.0|[0.21461530716697...|\n",
            "|       D|          3.0|       1.0|[0.20526304147140...|\n",
            "+--------+-------------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Preprocesamiento\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\", \"f4\"], outputCol=\"features\")\n",
        "\n",
        "# Dividir los datos\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Crear y entrenar el modelo\n",
        "lr_multi = LogisticRegression(featuresCol=\"features\", labelCol=\"categoryIndex\", maxIter=10, regParam=0.3)\n",
        "pipeline_multi = Pipeline(stages=[indexer, assembler, lr_multi])\n",
        "model_multi = pipeline_multi.fit(train_data)\n",
        "\n",
        "# Evaluar el modelo\n",
        "predictions_multi = model_multi.transform(test_data)\n",
        "evaluator_multi = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_multi.evaluate(predictions_multi)\n",
        "print(f\"\\nEjemplo 3: Clasificación multiclase\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Matriz de confusión\n",
        "print(\"Matriz de confusión:\")\n",
        "predictions_multi.groupBy(\"categoryIndex\", \"prediction\").count().show()\n",
        "\n",
        "# Mostrar algunas predicciones\n",
        "print(\"\\nAlgunas predicciones:\")\n",
        "predictions_multi.select(\"category\", \"categoryIndex\", \"prediction\", \"probability\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPzujX-JZyhs"
      },
      "source": [
        "## Ejercicios resueltos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbaqGX5Zyhs"
      },
      "source": [
        "### Ejercicio 1: Clasificación de Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b4hd8ycZyhs",
        "outputId": "83745f31-4ecf-4427-e5f6-86af20ec99a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|  1.0|       1.0|   13|\n",
            "|  0.0|       0.0|    6|\n",
            "|  2.0|       2.0|   13|\n",
            "+-----+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Cargar el dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
        "iris_pd = pd.read_csv(url, names=names)\n",
        "iris_df = spark.createDataFrame(iris_pd)\n",
        "\n",
        "# Preprocesamiento\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "assembler = VectorAssembler(inputCols=[\"sepal-length\", \"sepal-width\", \"petal-length\", \"petal-width\"], outputCol=\"features\")\n",
        "\n",
        "# Dividir los datos\n",
        "train_data, test_data = iris_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Crear y entrenar el modelo\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Evaluar el modelo\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Matriz de confusión\n",
        "predictions.groupBy(\"label\", \"prediction\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8wW1OLgZyhs"
      },
      "source": [
        "### Ejercicio 2: Predicción de precios de viviendas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PpKmMOtoZyht",
        "outputId": "cb47985b-fd5b-4573-a309-4cd8222f1372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 56319.05910152569\n",
            "R2: 0.7657823584175962\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler, Imputer\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HousingPrediction\").getOrCreate()\n",
        "\n",
        "# Cargar el dataset\n",
        "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
        "housing_pd = pd.read_csv(url)\n",
        "housing_df = spark.createDataFrame(housing_pd)\n",
        "\n",
        "# Preprocesamiento\n",
        "numeric_cols = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
        "categorical_cols = [\"ocean_proximity\"]\n",
        "\n",
        "# Manejar valores faltantes\n",
        "imputer = Imputer(inputCols=[\"total_bedrooms\"], outputCols=[\"total_bedrooms_imputed\"])\n",
        "\n",
        "# Codificar variables categóricas\n",
        "indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n",
        "encoder = OneHotEncoder(inputCols=[\"ocean_proximity_index\"], outputCols=[\"ocean_proximity_vec\"])\n",
        "\n",
        "# Combinar características numéricas y categóricas\n",
        "assembler_inputs = [col + \"_imputed\" if col == \"total_bedrooms\" else col for col in numeric_cols] + [\"ocean_proximity_vec\"]\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_unscaled\")\n",
        "\n",
        "# Escalar características\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withStd=True, withMean=False)\n",
        "\n",
        "# Modelo\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"median_house_value\")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, rf])\n",
        "\n",
        "# Dividir los datos\n",
        "train_data, test_data = housing_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Configurar CrossValidator\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 20]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3)\n",
        "\n",
        "# Entrenar el modelo\n",
        "cv_model = cv.fit(train_data)\n",
        "\n",
        "# Hacer predicciones\n",
        "predictions = cv_model.transform(test_data)\n",
        "\n",
        "# Evaluar el modelo\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "r2_evaluator = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = r2_evaluator.evaluate(predictions)\n",
        "print(f\"R2: {r2}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
